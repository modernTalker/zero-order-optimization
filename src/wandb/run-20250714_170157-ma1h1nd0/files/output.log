2025-07-14 17:02:09,472 - INFO - Sample train set 1500/67349
2025-07-14 17:02:09,472 - INFO - ... including dev set 500 samples
2025-07-14 17:02:09,473 - INFO - Loading model with FP16...
8
2025-07-14 17:02:15,212 - INFO - Done with 5.74s
2025-07-14 17:02:15,558 - INFO - Dev samples: 500
2025-07-14 17:02:15,558 - INFO - Train samples: 1000
2025-07-14 17:02:15,558 - INFO - Eval sample length is 872
2025-07-14 17:02:15,559 - INFO - Tokenizing training samples...
2025-07-14 17:02:16,449 - INFO - Done with 0.89s
/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-14 17:02:16,463 - INFO - ***** Running training *****
2025-07-14 17:02:16,463 - INFO -   Num examples = 1000
2025-07-14 17:02:16,463 - INFO -   Num Epochs = 318
2025-07-14 17:02:16,463 - INFO -   Instantaneous batch size per device = 16
2025-07-14 17:02:16,463 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-14 17:02:16,463 - INFO -   Gradient Accumulation steps = 1
2025-07-14 17:02:16,463 - INFO -   Total optimization steps = 20000
2025-07-14 17:02:16,464 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                                                         | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
  0%|                                                                                                                                                                              | 1/20000 [00:02<13:26:33,  2.42s/it]

  0%|                                                                                                                                                                              | 2/20000 [00:04<12:20:00,  2.22s/it]

  0%|                                                                                                                                                                              | 3/20000 [00:06<11:35:25,  2.09s/it]

  0%|                                                                                                                                                                              | 4/20000 [00:08<11:20:36,  2.04s/it]

  0%|                                                                                                                                                                              | 5/20000 [00:10<11:19:50,  2.04s/it]

  0%|                                                                                                                                                                              | 6/20000 [00:12<11:08:28,  2.01s/it]

  0%|                                                                                                                                                                              | 7/20000 [00:14<10:54:55,  1.97s/it]

  0%|                                                                                                                                                                              | 8/20000 [00:16<11:01:18,  1.98s/it]

  0%|                                                                                                                                                                              | 9/20000 [00:18<11:02:23,  1.99s/it]

  0%|                                                                                                                                                                             | 10/20000 [00:20<10:34:49,  1.91s/it]
{'loss': 0.8281, 'learning_rate': 0.5, 'epoch': 0.16}
  0%|                                                                                                                                                                             | 11/20000 [00:22<11:00:18,  1.98s/it]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 543, in _inner_training_loop
    tr_loss_step = self.optimizer.step(closure) # FIXME: the same for other optimizers
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_muon.py", line 47, in step
    loss1 = closure()
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 764, in closure
    def closure(): return self.zo_forward(model, inputs)
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 778, in zo_forward
    inputs = self._prepare_inputs(inputs)
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 2637, in _prepare_inputs
    inputs = self._prepare_input(inputs)
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 2619, in _prepare_input
    return type(data)({k: self._prepare_input(v) for k, v in data.items()})
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 2619, in <dictcomp>
    return type(data)({k: self._prepare_input(v) for k, v in data.items()})
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 2629, in _prepare_input
    return data.to(**kwargs)
KeyboardInterrupt
{'peak_mem': 3.165524959564209, 'step_consumption': 2136.18540763855, 'epoch': 0.17}