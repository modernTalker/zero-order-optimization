2025-07-11 12:00:43,338 - INFO - Sample train set 1500/67349
2025-07-11 12:00:43,339 - INFO - ... including dev set 500 samples
2025-07-11 12:00:43,339 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
37
2025-07-11 12:00:48,311 - INFO - Done with 4.97s
2025-07-11 12:00:48,663 - INFO - Dev samples: 500
2025-07-11 12:00:48,664 - INFO - Train samples: 1000
2025-07-11 12:00:48,664 - INFO - Eval sample length is 872
2025-07-11 12:00:48,664 - INFO - Tokenizing training samples...
2025-07-11 12:00:49,510 - INFO - Done with 0.85s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-11 12:00:49,524 - INFO - ***** Running training *****
2025-07-11 12:00:49,524 - INFO -   Num examples = 1000
2025-07-11 12:00:49,524 - INFO -   Num Epochs = 318
2025-07-11 12:00:49,524 - INFO -   Instantaneous batch size per device = 16
2025-07-11 12:00:49,524 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-11 12:00:49,524 - INFO -   Gradient Accumulation steps = 1
2025-07-11 12:00:49,524 - INFO -   Total optimization steps = 20000
2025-07-11 12:00:49,525 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                                             | 2/20000 [00:01<2:50:58,  1.95it/s]
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
{'peak_mem': 7.485307216644287, 'step_consumption': 862.0550632476807, 'epoch': 0.02}
{'peak_mem': 8.127435684204102, 'step_consumption': 263.9355659484863, 'epoch': 0.03}
{'peak_mem': 8.127458572387695, 'step_consumption': 242.3098087310791, 'epoch': 0.05}
{'peak_mem': 8.127458572387695, 'step_consumption': 234.01451110839844, 'epoch': 0.06}

  0%|                                                                                                                                            | 11/20000 [00:03<1:22:06,  4.06it/s]
{'peak_mem': 8.127481460571289, 'step_consumption': 233.60037803649902, 'epoch': 0.1}
{'peak_mem': 8.127504348754883, 'step_consumption': 233.12067985534668, 'epoch': 0.11}
{'peak_mem': 8.127504348754883, 'step_consumption': 227.33449935913086, 'epoch': 0.13}
{'peak_mem': 8.127527236938477, 'step_consumption': 233.1368923187256, 'epoch': 0.14}
{'loss': 0.4348, 'learning_rate': 0.5, 'epoch': 0.16}
{'peak_mem': 8.127527236938477, 'step_consumption': 242.7210807800293, 'epoch': 0.16}
{'peak_mem': 8.12755012512207, 'step_consumption': 242.71893501281738, 'epoch': 0.17}
{'peak_mem': 8.12755012512207, 'step_consumption': 245.0270652770996, 'epoch': 0.19}

  0%|▏                                                                                                                                           | 19/20000 [00:05<1:21:58,  4.06it/s]
{'peak_mem': 8.127573013305664, 'step_consumption': 245.6045150756836, 'epoch': 0.22}
{'peak_mem': 8.127595901489258, 'step_consumption': 238.2946014404297, 'epoch': 0.24}
{'peak_mem': 8.127603530883789, 'step_consumption': 227.9815673828125, 'epoch': 0.25}
{'peak_mem': 8.12761116027832, 'step_consumption': 234.99608039855957, 'epoch': 0.27}
{'peak_mem': 8.186124801635742, 'step_consumption': 252.1953582763672, 'epoch': 0.29}
{'peak_mem': 8.186124801635742, 'step_consumption': 250.22196769714355, 'epoch': 0.3}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.32}
{'peak_mem': 8.186124801635742, 'step_consumption': 242.29979515075684, 'epoch': 0.32}

  0%|▏                                                                                                                                           | 28/20000 [00:07<1:18:52,  4.22it/s]
{'peak_mem': 8.186124801635742, 'step_consumption': 241.9886589050293, 'epoch': 0.35}
{'peak_mem': 8.186124801635742, 'step_consumption': 224.72047805786133, 'epoch': 0.37}
{'peak_mem': 8.186124801635742, 'step_consumption': 225.71349143981934, 'epoch': 0.38}
{'peak_mem': 8.186124801635742, 'step_consumption': 236.41157150268555, 'epoch': 0.4}
{'peak_mem': 8.186124801635742, 'step_consumption': 226.24802589416504, 'epoch': 0.41}
{'peak_mem': 8.18622350692749, 'step_consumption': 252.30669975280762, 'epoch': 0.43}
{'peak_mem': 8.18622350692749, 'step_consumption': 227.25582122802734, 'epoch': 0.44}
{'peak_mem': 8.18622350692749, 'step_consumption': 242.51770973205566, 'epoch': 0.46}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.48}

  0%|▎                                                                                                                                           | 36/20000 [00:09<1:21:46,  4.07it/s]
{'peak_mem': 8.18622350692749, 'step_consumption': 233.89291763305664, 'epoch': 0.49}
{'peak_mem': 8.18622350692749, 'step_consumption': 241.1961555480957, 'epoch': 0.51}
{'peak_mem': 8.305082321166992, 'step_consumption': 257.0650577545166, 'epoch': 0.52}
{'peak_mem': 8.305082321166992, 'step_consumption': 227.54597663879395, 'epoch': 0.54}
{'peak_mem': 8.30508279800415, 'step_consumption': 258.1641674041748, 'epoch': 0.56}
{'peak_mem': 8.30508279800415, 'step_consumption': 242.31433868408203, 'epoch': 0.57}
{'peak_mem': 8.30508279800415, 'step_consumption': 234.09366607666016, 'epoch': 0.59}

  0%|▎                                                                                                                                           | 44/20000 [00:11<1:17:03,  4.32it/s]
{'peak_mem': 8.30508279800415, 'step_consumption': 227.93006896972656, 'epoch': 0.62}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.63}
{'peak_mem': 8.30508279800415, 'step_consumption': 241.2731647491455, 'epoch': 0.63}
{'peak_mem': 8.30508279800415, 'step_consumption': 228.1794548034668, 'epoch': 0.65}
{'peak_mem': 8.30508279800415, 'step_consumption': 235.0451946258545, 'epoch': 0.67}
{'peak_mem': 8.30508279800415, 'step_consumption': 226.3200283050537, 'epoch': 0.68}
{'peak_mem': 8.30508279800415, 'step_consumption': 223.35076332092285, 'epoch': 0.7}
{'peak_mem': 8.30508279800415, 'step_consumption': 241.82486534118652, 'epoch': 0.71}
{'peak_mem': 8.30508279800415, 'step_consumption': 233.98780822753906, 'epoch': 0.73}

  0%|▎                                                                                                                                           | 53/20000 [00:13<1:19:32,  4.18it/s]
{'peak_mem': 8.30508279800415, 'step_consumption': 233.75320434570312, 'epoch': 0.76}
{'peak_mem': 8.30508279800415, 'step_consumption': 233.48045349121094, 'epoch': 0.78}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.79}
{'peak_mem': 8.30508279800415, 'step_consumption': 234.938383102417, 'epoch': 0.79}
{'peak_mem': 8.30508279800415, 'step_consumption': 242.79141426086426, 'epoch': 0.81}
{'peak_mem': 8.30508279800415, 'step_consumption': 233.64782333374023, 'epoch': 0.83}
{'peak_mem': 8.30508279800415, 'step_consumption': 241.32251739501953, 'epoch': 0.84}
  0%|▍                                                                                                                                           | 54/20000 [00:13<1:20:02,  4.15it/s]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 520, in _inner_training_loop
    tr_loss_step = self.optimizer.step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_adam.py", line 24, in step
    return self.zo_step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_adam.py", line 51, in zo_step
    loss1 = self.trainer.zo_forward(model, inputs)
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 767, in zo_forward
    loss = self.compute_loss(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/utils.py", line 103, in forward_wrap_with_option_len
    if any([x != num_options[0] for x in num_options]):
KeyboardInterrupt