2025-07-14 17:39:46,984 - INFO - Sample train set 1500/67349
2025-07-14 17:39:46,985 - INFO - ... including dev set 500 samples
2025-07-14 17:39:46,985 - INFO - Loading model with FP16...
8
2025-07-14 17:39:53,003 - INFO - Done with 6.02s
2025-07-14 17:39:53,594 - INFO - Dev samples: 500
2025-07-14 17:39:53,594 - INFO - Train samples: 1000
2025-07-14 17:39:53,594 - INFO - Eval sample length is 872
2025-07-14 17:39:53,595 - INFO - Tokenizing training samples...
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
2025-07-14 17:39:54,502 - INFO - Done with 0.91s
/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-14 17:39:54,524 - INFO - ***** Running training *****
2025-07-14 17:39:54,524 - INFO -   Num examples = 1000
2025-07-14 17:39:54,524 - INFO -   Num Epochs = 318
2025-07-14 17:39:54,524 - INFO -   Instantaneous batch size per device = 16
2025-07-14 17:39:54,524 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-14 17:39:54,524 - INFO -   Gradient Accumulation steps = 1
2025-07-14 17:39:54,524 - INFO -   Total optimization steps = 20000
2025-07-14 17:39:54,525 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                                                         | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'peak_mem': 3.474902629852295, 'step_consumption': 160469.64263916016, 'epoch': 0.02}
  0%|                                                                                                                                                                            | 1/20000 [02:40<891:28:25, 160.47s/it]

