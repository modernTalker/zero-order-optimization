2025-07-11 14:14:57,998 - INFO - Sample train set 1500/67349
2025-07-11 14:14:57,999 - INFO - ... including dev set 500 samples
2025-07-11 14:14:57,999 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
37
2025-07-11 14:15:02,310 - INFO - Done with 4.31s
2025-07-11 14:15:02,628 - INFO - Dev samples: 500
2025-07-11 14:15:02,628 - INFO - Train samples: 1000
2025-07-11 14:15:02,628 - INFO - Eval sample length is 872
2025-07-11 14:15:02,629 - INFO - Tokenizing training samples...
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
2025-07-11 14:15:03,512 - INFO - Done with 0.88s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-11 14:15:03,525 - INFO - ***** Running training *****
2025-07-11 14:15:03,525 - INFO -   Num examples = 1000
2025-07-11 14:15:03,525 - INFO -   Num Epochs = 318
2025-07-11 14:15:03,525 - INFO -   Instantaneous batch size per device = 16
2025-07-11 14:15:03,525 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-11 14:15:03,525 - INFO -   Gradient Accumulation steps = 1
2025-07-11 14:15:03,525 - INFO -   Total optimization steps = 20000
2025-07-11 14:15:03,526 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                                             | 4/20000 [00:01<2:20:44,  2.37it/s]
{'peak_mem': 5.484857559204102, 'step_consumption': 838.0696773529053, 'epoch': 0.02}
{'peak_mem': 5.497550964355469, 'step_consumption': 360.04161834716797, 'epoch': 0.03}
{'peak_mem': 5.616346836090088, 'step_consumption': 362.4424934387207, 'epoch': 0.05}
{'peak_mem': 5.616346836090088, 'step_consumption': 342.7262306213379, 'epoch': 0.06}

  0%|                                                                                                                                            | 10/20000 [00:03<1:56:05,  2.87it/s]
{'peak_mem': 5.616346836090088, 'step_consumption': 354.6762466430664, 'epoch': 0.1}
{'peak_mem': 5.616346836090088, 'step_consumption': 345.0777530670166, 'epoch': 0.11}
{'peak_mem': 5.616346836090088, 'step_consumption': 327.099084854126, 'epoch': 0.13}
{'peak_mem': 5.616346836090088, 'step_consumption': 332.3674201965332, 'epoch': 0.14}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.16}

  0%|                                                                                                                                            | 15/20000 [00:05<1:58:04,  2.82it/s]
{'peak_mem': 5.616346836090088, 'step_consumption': 364.72272872924805, 'epoch': 0.17}
{'peak_mem': 5.616346836090088, 'step_consumption': 362.2400760650635, 'epoch': 0.19}
{'peak_mem': 5.616346836090088, 'step_consumption': 356.0328483581543, 'epoch': 0.21}
{'peak_mem': 5.616346836090088, 'step_consumption': 361.85550689697266, 'epoch': 0.22}
{'peak_mem': 5.616346836090088, 'step_consumption': 338.70840072631836, 'epoch': 0.24}

  0%|▏                                                                                                                                           | 21/20000 [00:07<1:59:30,  2.79it/s]
{'peak_mem': 5.616346836090088, 'step_consumption': 338.61732482910156, 'epoch': 0.27}
{'peak_mem': 5.735144138336182, 'step_consumption': 384.0973377227783, 'epoch': 0.29}
{'peak_mem': 5.735144138336182, 'step_consumption': 355.4360866546631, 'epoch': 0.3}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.32}
{'peak_mem': 5.735144138336182, 'step_consumption': 360.7332706451416, 'epoch': 0.32}
{'peak_mem': 5.735144138336182, 'step_consumption': 357.0868968963623, 'epoch': 0.33}

  0%|▏                                                                                                                                           | 27/20000 [00:09<1:56:18,  2.86it/s]
{'peak_mem': 5.735144138336182, 'step_consumption': 312.12759017944336, 'epoch': 0.37}
{'peak_mem': 5.735144138336182, 'step_consumption': 319.8537826538086, 'epoch': 0.38}
{'peak_mem': 5.735144138336182, 'step_consumption': 338.91916275024414, 'epoch': 0.4}
{'peak_mem': 5.735144138336182, 'step_consumption': 321.60139083862305, 'epoch': 0.41}
{'peak_mem': 5.73514461517334, 'step_consumption': 382.037878036499, 'epoch': 0.43}

  0%|▏                                                                                                                                           | 32/20000 [00:11<1:54:57,  2.89it/s]
{'peak_mem': 5.73514461517334, 'step_consumption': 356.9481372833252, 'epoch': 0.46}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.48}
{'peak_mem': 5.73514461517334, 'step_consumption': 321.6893672943115, 'epoch': 0.48}
{'peak_mem': 5.73514461517334, 'step_consumption': 338.59753608703613, 'epoch': 0.49}
{'peak_mem': 5.73514461517334, 'step_consumption': 355.88693618774414, 'epoch': 0.51}

  0%|▎                                                                                                                                           | 38/20000 [00:13<1:59:05,  2.79it/s]
{'peak_mem': 5.854625701904297, 'step_consumption': 319.166898727417, 'epoch': 0.54}
{'peak_mem': 5.854625701904297, 'step_consumption': 399.4452953338623, 'epoch': 0.56}
{'peak_mem': 5.854625701904297, 'step_consumption': 357.6774597167969, 'epoch': 0.57}
{'peak_mem': 5.854625701904297, 'step_consumption': 342.1971797943115, 'epoch': 0.59}
{'peak_mem': 5.854625701904297, 'step_consumption': 356.1086654663086, 'epoch': 0.6}
  0%|▎                                                                                                                                           | 42/20000 [00:15<2:00:07,  2.77it/s]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 548, in _inner_training_loop
    tr_loss_step = self.optimizer.step(closure)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_conserv.py", line 85, in step
    update_params(sign=-2.0)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_conserv.py", line 78, in update_params
    self._inner_optimizer.step()
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/sgd.py", line 73, in step
    has_sparse_grad = self._init_group(group, params_with_grad, d_p_list, momentum_buffer_list)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/sgd.py", line 40, in _init_group
    if p.grad is not None:
KeyboardInterrupt
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.63}
{'peak_mem': 5.854625701904297, 'step_consumption': 472.0141887664795, 'epoch': 0.63}
{'peak_mem': 5.854625701904297, 'step_consumption': 324.99074935913086, 'epoch': 0.65}
{'peak_mem': 5.854625701904297, 'step_consumption': 343.72448921203613, 'epoch': 0.67}