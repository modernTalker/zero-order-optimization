2025-07-11 11:30:33,118 - INFO - Sample train set 1500/67349
2025-07-11 11:30:33,119 - INFO - ... including dev set 500 samples
2025-07-11 11:30:33,119 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
12
2025-07-11 11:30:38,025 - INFO - Done with 4.91s
2025-07-11 11:30:38,330 - INFO - Dev samples: 500
2025-07-11 11:30:38,331 - INFO - Train samples: 1000
2025-07-11 11:30:38,331 - INFO - Eval sample length is 872
2025-07-11 11:30:38,331 - INFO - Tokenizing training samples...
2025-07-11 11:30:39,186 - INFO - Done with 0.85s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-11 11:30:39,201 - INFO - ***** Running training *****
2025-07-11 11:30:39,201 - INFO -   Num examples = 1000
2025-07-11 11:30:39,201 - INFO -   Num Epochs = 318
2025-07-11 11:30:39,201 - INFO -   Instantaneous batch size per device = 16
2025-07-11 11:30:39,202 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-11 11:30:39,202 - INFO -   Gradient Accumulation steps = 1
2025-07-11 11:30:39,202 - INFO -   Total optimization steps = 20000
2025-07-11 11:30:39,203 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                                             | 4/20000 [00:01<1:30:42,  3.67it/s]
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
{'peak_mem': 7.396742820739746, 'step_consumption': 626.3258457183838, 'epoch': 0.02}
{'peak_mem': 7.427903652191162, 'step_consumption': 217.0422077178955, 'epoch': 0.03}
{'peak_mem': 7.427903652191162, 'step_consumption': 221.49968147277832, 'epoch': 0.05}
{'peak_mem': 7.427903652191162, 'step_consumption': 206.8793773651123, 'epoch': 0.06}
{'peak_mem': 7.427903652191162, 'step_consumption': 199.60761070251465, 'epoch': 0.08}

  0%|                                                                                                                                            | 14/20000 [00:03<1:10:47,  4.71it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 201.5092372894287, 'epoch': 0.11}
{'peak_mem': 7.427903652191162, 'step_consumption': 199.0070343017578, 'epoch': 0.13}
{'peak_mem': 7.427903652191162, 'step_consumption': 198.89569282531738, 'epoch': 0.14}
{'loss': 2.5685, 'learning_rate': 0.5, 'epoch': 0.16}
{'peak_mem': 7.427903652191162, 'step_consumption': 209.09428596496582, 'epoch': 0.16}
{'peak_mem': 7.427903652191162, 'step_consumption': 214.65229988098145, 'epoch': 0.17}
{'peak_mem': 7.427903652191162, 'step_consumption': 210.2487087249756, 'epoch': 0.19}
{'peak_mem': 7.427903652191162, 'step_consumption': 203.1564712524414, 'epoch': 0.21}
{'peak_mem': 7.427903652191162, 'step_consumption': 214.0350341796875, 'epoch': 0.22}
{'peak_mem': 7.427903652191162, 'step_consumption': 202.35943794250488, 'epoch': 0.24}

  0%|▏                                                                                                                                           | 24/20000 [00:05<1:07:58,  4.90it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 204.4358253479004, 'epoch': 0.27}
{'peak_mem': 7.427903652191162, 'step_consumption': 223.54769706726074, 'epoch': 0.29}
{'peak_mem': 7.427903652191162, 'step_consumption': 211.01832389831543, 'epoch': 0.3}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.32}
{'peak_mem': 7.427903652191162, 'step_consumption': 212.0661735534668, 'epoch': 0.32}
{'peak_mem': 7.427903652191162, 'step_consumption': 210.95538139343262, 'epoch': 0.33}
{'peak_mem': 7.427903652191162, 'step_consumption': 212.6009464263916, 'epoch': 0.35}
{'peak_mem': 7.427903652191162, 'step_consumption': 190.4621124267578, 'epoch': 0.37}
{'peak_mem': 7.427903652191162, 'step_consumption': 194.8709487915039, 'epoch': 0.38}

  0%|▏                                                                                                                                           | 32/20000 [00:07<1:29:27,  3.72it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 195.47200202941895, 'epoch': 0.41}
{'peak_mem': 7.427903652191162, 'step_consumption': 245.9392547607422, 'epoch': 0.43}
{'peak_mem': 7.427903652191162, 'step_consumption': 297.47796058654785, 'epoch': 0.44}
{'peak_mem': 7.427903652191162, 'step_consumption': 253.88669967651367, 'epoch': 0.46}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.48}
{'peak_mem': 7.427903652191162, 'step_consumption': 273.04911613464355, 'epoch': 0.48}
{'peak_mem': 7.427903652191162, 'step_consumption': 239.715576171875, 'epoch': 0.49}
{'peak_mem': 7.427903652191162, 'step_consumption': 308.68077278137207, 'epoch': 0.51}

  0%|▎                                                                                                                                           | 38/20000 [00:09<1:34:21,  3.53it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 232.54942893981934, 'epoch': 0.54}
{'peak_mem': 7.427903652191162, 'step_consumption': 376.4827251434326, 'epoch': 0.56}
{'peak_mem': 7.427903652191162, 'step_consumption': 240.71216583251953, 'epoch': 0.57}
{'peak_mem': 7.427903652191162, 'step_consumption': 314.2235279083252, 'epoch': 0.59}
{'peak_mem': 7.427903652191162, 'step_consumption': 251.78194046020508, 'epoch': 0.6}
{'peak_mem': 7.427903652191162, 'step_consumption': 277.62579917907715, 'epoch': 0.62}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.63}

  0%|▎                                                                                                                                           | 46/20000 [00:11<1:36:24,  3.45it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 284.3031883239746, 'epoch': 0.65}
{'peak_mem': 7.427903652191162, 'step_consumption': 229.1114330291748, 'epoch': 0.67}
{'peak_mem': 7.427903652191162, 'step_consumption': 290.0211811065674, 'epoch': 0.68}
{'peak_mem': 7.427903652191162, 'step_consumption': 235.69869995117188, 'epoch': 0.7}
{'peak_mem': 7.427903652191162, 'step_consumption': 311.44142150878906, 'epoch': 0.71}
{'peak_mem': 7.427903652191162, 'step_consumption': 317.7990913391113, 'epoch': 0.73}

  0%|▎                                                                                                                                           | 53/20000 [00:13<1:26:49,  3.83it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 314.4645690917969, 'epoch': 0.76}
{'peak_mem': 7.427903652191162, 'step_consumption': 223.88219833374023, 'epoch': 0.78}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.79}
{'peak_mem': 7.427903652191162, 'step_consumption': 318.8133239746094, 'epoch': 0.79}
{'peak_mem': 7.427903652191162, 'step_consumption': 216.34435653686523, 'epoch': 0.81}
{'peak_mem': 7.427903652191162, 'step_consumption': 317.89112091064453, 'epoch': 0.83}
{'peak_mem': 7.427903652191162, 'step_consumption': 213.26851844787598, 'epoch': 0.84}

  0%|▍                                                                                                                                           | 61/20000 [00:15<1:26:57,  3.82it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 248.09646606445312, 'epoch': 0.87}
{'peak_mem': 7.427903652191162, 'step_consumption': 296.8618869781494, 'epoch': 0.89}
{'peak_mem': 7.427903652191162, 'step_consumption': 221.2226390838623, 'epoch': 0.9}
{'peak_mem': 7.427903652191162, 'step_consumption': 313.14849853515625, 'epoch': 0.92}
{'peak_mem': 7.427903652191162, 'step_consumption': 229.04729843139648, 'epoch': 0.94}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.95}
{'peak_mem': 7.427903652191162, 'step_consumption': 281.85510635375977, 'epoch': 0.95}
{'peak_mem': 7.427903652191162, 'step_consumption': 237.9891872406006, 'epoch': 0.97}

  0%|▍                                                                                                                                           | 68/20000 [00:17<1:31:38,  3.63it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 268.9478397369385, 'epoch': 1.0}
-------------------------- Training Epoch 1 --------------------------
{'peak_mem': 7.427903652191162, 'step_consumption': 258.2378387451172, 'epoch': 1.02}
{'peak_mem': 7.427903652191162, 'step_consumption': 305.3851127624512, 'epoch': 1.03}
{'peak_mem': 7.427903652191162, 'step_consumption': 243.03174018859863, 'epoch': 1.05}
{'peak_mem': 7.427903652191162, 'step_consumption': 311.4950656890869, 'epoch': 1.06}
{'peak_mem': 7.427903652191162, 'step_consumption': 254.52876091003418, 'epoch': 1.08}

  0%|▌                                                                                                                                           | 74/20000 [00:19<1:42:46,  3.23it/s]
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 1.11}
{'peak_mem': 7.427903652191162, 'step_consumption': 311.7830753326416, 'epoch': 1.11}
{'peak_mem': 7.427903652191162, 'step_consumption': 246.85430526733398, 'epoch': 1.13}
{'peak_mem': 7.427903652191162, 'step_consumption': 309.2067241668701, 'epoch': 1.14}
{'peak_mem': 7.427903652191162, 'step_consumption': 327.0525932312012, 'epoch': 1.16}
{'peak_mem': 7.427903652191162, 'step_consumption': 320.253849029541, 'epoch': 1.17}

  0%|▌                                                                                                                                           | 81/20000 [00:21<1:35:26,  3.48it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 265.0740146636963, 'epoch': 1.21}
{'peak_mem': 7.427903652191162, 'step_consumption': 290.62366485595703, 'epoch': 1.22}
{'peak_mem': 7.427903652191162, 'step_consumption': 352.7874946594238, 'epoch': 1.24}
{'peak_mem': 7.427903652191162, 'step_consumption': 250.75888633728027, 'epoch': 1.25}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 1.27}
{'peak_mem': 7.427903652191162, 'step_consumption': 316.27798080444336, 'epoch': 1.27}
{'peak_mem': 7.427903652191162, 'step_consumption': 244.34423446655273, 'epoch': 1.29}

  0%|▌                                                                                                                                           | 89/20000 [00:23<1:18:14,  4.24it/s]
{'peak_mem': 7.427903652191162, 'step_consumption': 279.6308994293213, 'epoch': 1.32}
{'peak_mem': 7.427903652191162, 'step_consumption': 223.99258613586426, 'epoch': 1.33}
{'peak_mem': 7.427903652191162, 'step_consumption': 223.9975929260254, 'epoch': 1.35}
{'peak_mem': 7.427903652191162, 'step_consumption': 227.32973098754883, 'epoch': 1.37}
{'peak_mem': 7.427903652191162, 'step_consumption': 224.3661880493164, 'epoch': 1.38}
{'peak_mem': 7.427903652191162, 'step_consumption': 220.84808349609375, 'epoch': 1.4}
{'peak_mem': 7.427903652191162, 'step_consumption': 228.29961776733398, 'epoch': 1.41}
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 1.43}
  0%|▋                                                                                                                                           | 90/20000 [00:23<1:16:50,  4.32it/s]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 540, in _inner_training_loop
    tr_loss_step = self.optimizer.step(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/jaguar_sign_sgd.py", line 61, in step
    loss2 = self.trainer.zo_forward(model, inputs)
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 773, in zo_forward
    loss = self.compute_loss(model, inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/utils.py", line 67, in forward_wrap_with_option_len
    outputs = self.original_forward(input_ids=input_ids, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 938, in forward
    outputs = self.model.decoder(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 645, in forward
    causal_attention_mask = self._prepare_decoder_attention_mask(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 538, in _prepare_decoder_attention_mask
    combined_attention_mask = _make_causal_mask(
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 77, in _make_causal_mask
    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
KeyboardInterrupt