2025-07-17 14:27:08,217 - INFO - Sample train set 1500/67349
2025-07-17 14:27:08,218 - INFO - ... including dev set 500 samples
2025-07-17 14:27:08,218 - INFO - Loading model with FP16...
38
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.08s/it]
2025-07-17 14:27:19,594 - INFO - Done with 11.38s
2025-07-17 14:27:19,868 - INFO - Dev samples: 500
2025-07-17 14:27:19,871 - INFO - Train samples: 1000
2025-07-17 14:27:19,872 - INFO - Eval sample length is 872
2025-07-17 14:27:19,872 - INFO - Tokenizing training samples...
2025-07-17 14:27:20,818 - INFO - Done with 0.94s
/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-17 14:27:20,836 - INFO - ***** Running training *****
2025-07-17 14:27:20,837 - INFO -   Num examples = 1000
2025-07-17 14:27:20,838 - INFO -   Num Epochs = 318
2025-07-17 14:27:20,839 - INFO -   Instantaneous batch size per device = 16
2025-07-17 14:27:20,839 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-17 14:27:20,840 - INFO -   Gradient Accumulation steps = 1
2025-07-17 14:27:20,841 - INFO -   Total optimization steps = 20000
2025-07-17 14:27:20,843 - INFO -   Number of trainable parameters = 6738415616
  0%|                                                                                                                                                                                         | 0/20000 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
{'peak_mem': 14.393212795257568, 'step_consumption': 1091.0890102386475, 'epoch': 0.02}
  0%|                                                                                                                                                                               | 3/20000 [00:02<4:13:16,  1.32it/s]
{'peak_mem': 14.393213272094727, 'step_consumption': 628.4499168395996, 'epoch': 0.03}
{'peak_mem': 14.393213272094727, 'step_consumption': 680.5572509765625, 'epoch': 0.05}

  0%|                                                                                                                                                                               | 6/20000 [00:04<3:40:50,  1.51it/s]
{'peak_mem': 14.393213272094727, 'step_consumption': 594.1591262817383, 'epoch': 0.08}
  0%|                                                                                                                                                                               | 6/20000 [00:04<3:40:50,  1.51it/s]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 741, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 693, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 578, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm2/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 568, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt