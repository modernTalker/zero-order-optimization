2025-07-11 14:58:40,048 - INFO - Sample train set 1500/67349
2025-07-11 14:58:40,049 - INFO - ... including dev set 500 samples
2025-07-11 14:58:40,049 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
37
2025-07-11 14:58:44,523 - INFO - Done with 4.47s
2025-07-11 14:58:44,805 - INFO - Dev samples: 500
2025-07-11 14:58:44,806 - INFO - Train samples: 1000
2025-07-11 14:58:44,806 - INFO - Eval sample length is 872
2025-07-11 14:58:44,806 - INFO - Tokenizing training samples...
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
2025-07-11 14:58:45,680 - INFO - Done with 0.87s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-11 14:58:45,691 - INFO - ***** Running training *****
2025-07-11 14:58:45,691 - INFO -   Num examples = 1000
2025-07-11 14:58:45,692 - INFO -   Num Epochs = 318
2025-07-11 14:58:45,692 - INFO -   Instantaneous batch size per device = 16
2025-07-11 14:58:45,692 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-11 14:58:45,692 - INFO -   Gradient Accumulation steps = 1
2025-07-11 14:58:45,692 - INFO -   Total optimization steps = 20000
2025-07-11 14:58:45,693 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|                                                                                                                                           | 1/20000 [00:47<265:36:10, 47.81s/it]

  0%|                                                                                                                                           | 2/20000 [01:35<263:55:06, 47.51s/it]

  0%|                                                                                                                                           | 3/20000 [02:22<263:50:43, 47.50s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47483.44612121582, 'epoch': 0.05}


  0%|                                                                                                                                           | 5/20000 [03:57<263:54:36, 47.52s/it]

  0%|                                                                                                                                           | 6/20000 [04:45<263:56:23, 47.52s/it]

  0%|                                                                                                                                           | 7/20000 [05:32<263:56:55, 47.53s/it]

  0%|                                                                                                                                           | 8/20000 [06:20<263:54:37, 47.52s/it]

  0%|                                                                                                                                           | 9/20000 [07:07<263:53:05, 47.52s/it]

  0%|                                                                                                                                          | 10/20000 [07:55<263:52:24, 47.52s/it]
{'loss': 0.5914, 'learning_rate': 0.5, 'epoch': 0.16}
{'peak_mem': 8.345207214355469, 'step_consumption': 47526.952266693115, 'epoch': 0.16}


  0%|                                                                                                                                          | 12/20000 [09:30<263:51:29, 47.52s/it]

  0%|                                                                                                                                          | 13/20000 [10:17<263:49:54, 47.52s/it]

  0%|                                                                                                                                          | 14/20000 [11:05<263:48:34, 47.52s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47512.813568115234, 'epoch': 0.22}


  0%|                                                                                                                                          | 16/20000 [12:40<263:44:40, 47.51s/it]

  0%|                                                                                                                                          | 17/20000 [13:27<263:44:50, 47.51s/it]

  0%|                                                                                                                                          | 18/20000 [14:15<263:45:54, 47.52s/it]

  0%|▏                                                                                                                                         | 19/20000 [15:02<263:43:43, 47.52s/it]

  0%|▏                                                                                                                                         | 20/20000 [15:50<263:41:44, 47.51s/it]
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.32}

  0%|▏                                                                                                                                         | 21/20000 [16:37<263:41:27, 47.51s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47514.424324035645, 'epoch': 0.33}


  0%|▏                                                                                                                                         | 23/20000 [18:12<263:35:08, 47.50s/it]

  0%|▏                                                                                                                                         | 24/20000 [19:00<263:32:21, 47.49s/it]

  0%|▏                                                                                                                                         | 25/20000 [19:47<263:31:22, 47.49s/it]

  0%|▏                                                                                                                                         | 26/20000 [20:35<263:30:54, 47.49s/it]

  0%|▏                                                                                                                                         | 27/20000 [21:22<263:35:19, 47.51s/it]

  0%|▏                                                                                                                                         | 28/20000 [22:10<263:41:27, 47.53s/it]

  0%|▏                                                                                                                                         | 29/20000 [22:58<263:42:20, 47.54s/it]

  0%|▏                                                                                                                                         | 30/20000 [23:45<263:52:18, 47.57s/it]
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.48}

  0%|▏                                                                                                                                         | 31/20000 [24:33<264:03:08, 47.60s/it]

  0%|▏                                                                                                                                         | 32/20000 [25:20<263:58:05, 47.59s/it]

  0%|▏                                                                                                                                         | 33/20000 [26:08<263:58:34, 47.59s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47600.37660598755, 'epoch': 0.52}


  0%|▏                                                                                                                                         | 35/20000 [27:44<264:17:30, 47.66s/it]

  0%|▏                                                                                                                                         | 36/20000 [28:31<264:17:37, 47.66s/it]

  0%|▎                                                                                                                                         | 37/20000 [29:19<264:38:33, 47.72s/it]

  0%|▎                                                                                                                                         | 38/20000 [30:07<264:20:52, 47.67s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47552.04367637634, 'epoch': 0.6}


  0%|▎                                                                                                                                         | 40/20000 [31:42<264:25:17, 47.69s/it]
{'loss': 0.0, 'learning_rate': 0.5, 'epoch': 0.63}

  0%|▎                                                                                                                                         | 41/20000 [32:30<264:21:41, 47.68s/it]

  0%|▎                                                                                                                                         | 42/20000 [33:18<264:31:40, 47.72s/it]

  0%|▎                                                                                                                                         | 43/20000 [34:05<264:23:33, 47.69s/it]
{'peak_mem': 8.345207214355469, 'step_consumption': 47639.20259475708, 'epoch': 0.68}

  0%|▎                                                                                                                                         | 44/20000 [34:53<264:04:42, 47.64s/it]


  0%|▎                                                                                                                                         | 46/20000 [36:28<263:43:51, 47.58s/it]

  0%|▎                                                                                                                                         | 47/20000 [37:15<263:37:59, 47.57s/it]
  0%|▎                                                                                                                                         | 47/20000 [37:15<263:37:59, 47.57s/it]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 544, in _inner_training_loop
    tr_loss_step = self.optimizer.step(closure)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_sampling_muon.py", line 44, in step
    zo_muon_perturb_parameters(1)
  File "/home/rinya/zero-order-optimization/src/optimizers/zo_sampling_muon.py", line 34, in zo_muon_perturb_parameters
    E = sample_ortho_approx(param.data.shape, device=self.device)
  File "/home/rinya/zero-order-optimization/src/optimizers/opt_utils/sampling.py", line 326, in sample_ortho_approx
    E = torch_ortho_rvs(p, device=device)
  File "/home/rinya/zero-order-optimization/src/optimizers/opt_utils/sampling.py", line 92, in torch_ortho_rvs
    q, r = torch.linalg.qr(z)
KeyboardInterrupt