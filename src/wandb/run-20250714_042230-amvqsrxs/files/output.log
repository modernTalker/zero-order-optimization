2025-07-14 04:22:41,282 - INFO - Sample train set 1500/67349
2025-07-14 04:22:41,283 - INFO - ... including dev set 500 samples
2025-07-14 04:22:41,284 - INFO - Loading model with FP16...
/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
10
2025-07-14 04:22:45,664 - INFO - Done with 4.38s
2025-07-14 04:22:46,016 - INFO - Dev samples: 500
2025-07-14 04:22:46,018 - INFO - Train samples: 1000
2025-07-14 04:22:46,019 - INFO - Eval sample length is 872
2025-07-14 04:22:46,020 - INFO - Tokenizing training samples...
2025-07-14 04:22:47,083 - INFO - Done with 1.06s
/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-14 04:22:47,101 - INFO - ***** Running training *****
2025-07-14 04:22:47,101 - INFO -   Num examples = 1000
2025-07-14 04:22:47,102 - INFO -   Num Epochs = 318
2025-07-14 04:22:47,103 - INFO -   Instantaneous batch size per device = 16
2025-07-14 04:22:47,104 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-14 04:22:47,105 - INFO -   Gradient Accumulation steps = 1
2025-07-14 04:22:47,106 - INFO -   Total optimization steps = 20000
2025-07-14 04:22:47,108 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                   | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
  0%|                                                                                         | 4/20000 [00:01<2:08:33,  2.59it/s]
{'peak_mem': 3.801133632659912, 'step_consumption': 592.3328399658203, 'epoch': 0.02}
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.4219169616699, 'epoch': 0.03}
{'peak_mem': 3.8011341094970703, 'step_consumption': 353.039026260376, 'epoch': 0.05}
{'peak_mem': 3.8011341094970703, 'step_consumption': 334.6898555755615, 'epoch': 0.06}
{'peak_mem': 3.8011341094970703, 'step_consumption': 286.588191986084, 'epoch': 0.08}

  0%|                                                                                        | 10/20000 [00:03<1:53:46,  2.93it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 334.02204513549805, 'epoch': 0.11}
{'peak_mem': 3.8011341094970703, 'step_consumption': 282.8338146209717, 'epoch': 0.13}
{'peak_mem': 3.8011341094970703, 'step_consumption': 354.2361259460449, 'epoch': 0.14}
{'loss': 0.8289, 'learning_rate': 0.5, 'epoch': 0.16}
{'peak_mem': 3.8011341094970703, 'step_consumption': 373.7187385559082, 'epoch': 0.16}
{'peak_mem': 3.8011341094970703, 'step_consumption': 374.08924102783203, 'epoch': 0.17}

  0%|                                                                                        | 15/20000 [00:05<1:59:45,  2.78it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 337.1388912200928, 'epoch': 0.21}
{'peak_mem': 3.8011341094970703, 'step_consumption': 375.46253204345703, 'epoch': 0.22}
{'peak_mem': 3.8011341094970703, 'step_consumption': 346.44484519958496, 'epoch': 0.24}

  0%|                                                                                        | 21/20000 [00:07<2:00:39,  2.76it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 338.1814956665039, 'epoch': 0.27}
{'peak_mem': 3.8011341094970703, 'step_consumption': 397.2456455230713, 'epoch': 0.29}
{'peak_mem': 3.8011341094970703, 'step_consumption': 352.08654403686523, 'epoch': 0.3}
{'loss': 0.9075, 'learning_rate': 0.5, 'epoch': 0.32}
{'peak_mem': 3.8011341094970703, 'step_consumption': 360.51225662231445, 'epoch': 0.32}

  0%|                                                                                        | 27/20000 [00:09<1:53:19,  2.94it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 352.0383834838867, 'epoch': 0.35}
{'peak_mem': 3.8011341094970703, 'step_consumption': 264.4836902618408, 'epoch': 0.37}
{'peak_mem': 3.8011341094970703, 'step_consumption': 305.42778968811035, 'epoch': 0.38}
{'peak_mem': 3.8011341094970703, 'step_consumption': 336.7879390716553, 'epoch': 0.4}
{'peak_mem': 3.8011341094970703, 'step_consumption': 287.5938415527344, 'epoch': 0.41}
{'peak_mem': 3.8011341094970703, 'step_consumption': 379.35423851013184, 'epoch': 0.43}

  0%|â–                                                                                       | 33/20000 [00:11<2:02:55,  2.71it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 365.4825687408447, 'epoch': 0.46}
{'loss': 0.859, 'learning_rate': 0.5, 'epoch': 0.48}
{'peak_mem': 3.8011341094970703, 'step_consumption': 337.74423599243164, 'epoch': 0.48}
{'peak_mem': 3.8011341094970703, 'step_consumption': 339.1683101654053, 'epoch': 0.49}
{'peak_mem': 3.8011341094970703, 'step_consumption': 365.82446098327637, 'epoch': 0.51}

  0%|â–                                                                                       | 38/20000 [00:13<2:09:47,  2.56it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 291.0153865814209, 'epoch': 0.54}
{'peak_mem': 3.8011341094970703, 'step_consumption': 451.54857635498047, 'epoch': 0.56}
{'peak_mem': 3.8011341094970703, 'step_consumption': 366.4557933807373, 'epoch': 0.57}
{'peak_mem': 3.8011341094970703, 'step_consumption': 338.93513679504395, 'epoch': 0.59}

  0%|â–                                                                                       | 44/20000 [00:15<1:45:11,  3.16it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 311.600923538208, 'epoch': 0.62}
{'loss': 0.8356, 'learning_rate': 0.5, 'epoch': 0.63}
{'peak_mem': 3.8011341094970703, 'step_consumption': 358.9615821838379, 'epoch': 0.63}
{'peak_mem': 3.8011341094970703, 'step_consumption': 287.9295349121094, 'epoch': 0.65}
{'peak_mem': 3.8011341094970703, 'step_consumption': 339.6937847137451, 'epoch': 0.67}
{'peak_mem': 3.8011341094970703, 'step_consumption': 295.9778308868408, 'epoch': 0.68}

  0%|â–                                                                                       | 49/20000 [00:17<2:04:34,  2.67it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 405.7040214538574, 'epoch': 0.71}
{'peak_mem': 3.8011341094970703, 'step_consumption': 337.9096984863281, 'epoch': 0.73}
{'peak_mem': 3.8011341094970703, 'step_consumption': 302.5646209716797, 'epoch': 0.75}
{'peak_mem': 3.8011341094970703, 'step_consumption': 396.3432312011719, 'epoch': 0.76}
{'peak_mem': 3.8011341094970703, 'step_consumption': 413.6991500854492, 'epoch': 0.78}
{'loss': 0.9475, 'learning_rate': 0.5, 'epoch': 0.79}

  0%|â–                                                                                       | 55/20000 [00:19<2:05:56,  2.64it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 357.241153717041, 'epoch': 0.81}
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.6505603790283, 'epoch': 0.83}
{'peak_mem': 3.8011341094970703, 'step_consumption': 360.001802444458, 'epoch': 0.84}
{'peak_mem': 3.8011341094970703, 'step_consumption': 364.44854736328125, 'epoch': 0.86}

  0%|â–Ž                                                                                       | 61/20000 [00:21<1:57:00,  2.84it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.1671657562256, 'epoch': 0.89}
{'peak_mem': 3.8011341094970703, 'step_consumption': 288.12313079833984, 'epoch': 0.9}
{'peak_mem': 3.8011341094970703, 'step_consumption': 353.35850715637207, 'epoch': 0.92}
{'peak_mem': 3.8011341094970703, 'step_consumption': 292.5565242767334, 'epoch': 0.94}
{'loss': 0.8932, 'learning_rate': 0.5, 'epoch': 0.95}
{'peak_mem': 3.8011341094970703, 'step_consumption': 354.36081886291504, 'epoch': 0.95}

  0%|â–Ž                                                                                       | 67/20000 [00:23<1:40:42,  3.30it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.79051208496094, 'epoch': 0.98}
{'peak_mem': 3.8011341094970703, 'step_consumption': 266.9095993041992, 'epoch': 1.0}
-------------------------- Training Epoch 1 --------------------------
{'peak_mem': 3.8011341094970703, 'step_consumption': 291.90802574157715, 'epoch': 1.02}
{'peak_mem': 3.8011341094970703, 'step_consumption': 267.5662040710449, 'epoch': 1.03}
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.9742126464844, 'epoch': 1.05}
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.5205020904541, 'epoch': 1.06}

  0%|â–Ž                                                                                       | 73/20000 [00:25<1:51:03,  2.99it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 357.16867446899414, 'epoch': 1.1}
{'loss': 0.9579, 'learning_rate': 0.5, 'epoch': 1.11}
{'peak_mem': 3.8011341094970703, 'step_consumption': 273.3724117279053, 'epoch': 1.11}
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.6738052368164, 'epoch': 1.13}
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.9339199066162, 'epoch': 1.14}
{'peak_mem': 3.8011341094970703, 'step_consumption': 388.0894184112549, 'epoch': 1.16}

  0%|â–Ž                                                                                       | 79/20000 [00:27<1:56:30,  2.85it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.0199432373047, 'epoch': 1.19}
{'peak_mem': 3.8011341094970703, 'step_consumption': 359.4021797180176, 'epoch': 1.21}
{'peak_mem': 3.8011341094970703, 'step_consumption': 292.5846576690674, 'epoch': 1.22}
{'peak_mem': 3.8011341094970703, 'step_consumption': 354.9489974975586, 'epoch': 1.24}

  0%|â–Ž                                                                                       | 84/20000 [00:29<1:59:45,  2.77it/s]
{'loss': 0.9284, 'learning_rate': 0.5, 'epoch': 1.27}
{'peak_mem': 3.8011341094970703, 'step_consumption': 305.59372901916504, 'epoch': 1.27}
{'peak_mem': 3.8011341094970703, 'step_consumption': 384.9813938140869, 'epoch': 1.29}
{'peak_mem': 3.8011341094970703, 'step_consumption': 339.48206901550293, 'epoch': 1.3}
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.037109375, 'epoch': 1.32}
{'peak_mem': 3.8011341094970703, 'step_consumption': 358.0045700073242, 'epoch': 1.33}

  0%|â–                                                                                       | 90/20000 [00:31<1:58:30,  2.80it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 426.87296867370605, 'epoch': 1.37}
{'peak_mem': 3.8011341094970703, 'step_consumption': 340.01898765563965, 'epoch': 1.38}
{'peak_mem': 3.8011341094970703, 'step_consumption': 291.1872863769531, 'epoch': 1.4}
{'peak_mem': 3.8011341094970703, 'step_consumption': 347.96929359436035, 'epoch': 1.41}
{'loss': 0.9456, 'learning_rate': 0.5, 'epoch': 1.43}

  0%|â–                                                                                       | 95/20000 [00:33<1:55:50,  2.86it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 362.37287521362305, 'epoch': 1.44}
{'peak_mem': 3.8011341094970703, 'step_consumption': 386.3484859466553, 'epoch': 1.46}
{'peak_mem': 3.8011341094970703, 'step_consumption': 357.21421241760254, 'epoch': 1.48}
{'peak_mem': 3.8011341094970703, 'step_consumption': 358.25443267822266, 'epoch': 1.49}
{'peak_mem': 3.8011341094970703, 'step_consumption': 291.89133644104004, 'epoch': 1.51}

  1%|â–                                                                                      | 101/20000 [00:35<2:02:11,  2.71it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 355.8223247528076, 'epoch': 1.54}
{'peak_mem': 3.8011341094970703, 'step_consumption': 357.52081871032715, 'epoch': 1.56}
{'peak_mem': 3.8011341094970703, 'step_consumption': 364.75682258605957, 'epoch': 1.57}
{'loss': 0.8775, 'learning_rate': 0.5, 'epoch': 1.59}
{'peak_mem': 3.8011341094970703, 'step_consumption': 383.55469703674316, 'epoch': 1.59}
{'peak_mem': 3.8011341094970703, 'step_consumption': 346.9836711883545, 'epoch': 1.6}

  1%|â–                                                                                      | 107/20000 [00:37<2:01:36,  2.73it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 340.2712345123291, 'epoch': 1.63}
{'peak_mem': 3.8011341094970703, 'step_consumption': 290.1277542114258, 'epoch': 1.65}
{'peak_mem': 3.8011341094970703, 'step_consumption': 357.89012908935547, 'epoch': 1.67}
{'peak_mem': 3.8011341094970703, 'step_consumption': 407.44757652282715, 'epoch': 1.68}

  1%|â–                                                                                      | 112/20000 [00:39<2:02:04,  2.72it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 343.06979179382324, 'epoch': 1.71}
{'peak_mem': 3.8011341094970703, 'step_consumption': 358.889102935791, 'epoch': 1.73}
{'loss': 0.7884, 'learning_rate': 0.5, 'epoch': 1.75}
{'peak_mem': 3.8011341094970703, 'step_consumption': 351.5810966491699, 'epoch': 1.75}
{'peak_mem': 3.8011341094970703, 'step_consumption': 289.609432220459, 'epoch': 1.76}
{'peak_mem': 3.8011341094970703, 'step_consumption': 431.3352108001709, 'epoch': 1.78}

  1%|â–Œ                                                                                      | 118/20000 [00:41<1:50:08,  3.01it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 303.39741706848145, 'epoch': 1.81}
{'peak_mem': 3.8011341094970703, 'step_consumption': 300.2161979675293, 'epoch': 1.83}
{'peak_mem': 3.8011341094970703, 'step_consumption': 292.8581237792969, 'epoch': 1.84}
{'peak_mem': 3.8011341094970703, 'step_consumption': 382.83610343933105, 'epoch': 1.86}
{'peak_mem': 3.8011341094970703, 'step_consumption': 294.02780532836914, 'epoch': 1.87}

  1%|â–Œ                                                                                      | 124/20000 [00:43<1:56:04,  2.85it/s]
{'loss': 0.9175, 'learning_rate': 0.5, 'epoch': 1.9}
{'peak_mem': 3.8011341094970703, 'step_consumption': 343.3692455291748, 'epoch': 1.9}
{'peak_mem': 3.8011341094970703, 'step_consumption': 356.98866844177246, 'epoch': 1.92}
{'peak_mem': 3.8011341094970703, 'step_consumption': 321.4592933654785, 'epoch': 1.94}
{'peak_mem': 3.8011341094970703, 'step_consumption': 341.1750793457031, 'epoch': 1.95}
{'peak_mem': 3.8011341094970703, 'step_consumption': 363.0690574645996, 'epoch': 1.97}

  1%|â–Œ                                                                                      | 130/20000 [00:45<1:53:32,  2.92it/s]
{'peak_mem': 3.8011341094970703, 'step_consumption': 275.7134437561035, 'epoch': 2.0}
-------------------------- Training Epoch 2 --------------------------
{'peak_mem': 3.8011341094970703, 'step_consumption': 343.25432777404785, 'epoch': 2.02}
{'peak_mem': 3.8011341094970703, 'step_consumption': 345.57223320007324, 'epoch': 2.03}
{'peak_mem': 3.8011341094970703, 'step_consumption': 346.15302085876465, 'epoch': 2.05}
{'loss': 0.772, 'learning_rate': 0.5, 'epoch': 2.06}
{'peak_mem': 3.8011341094970703, 'step_consumption': 348.76155853271484, 'epoch': 2.06}
  1%|â–Œ                                                                                      | 131/20000 [00:45<1:49:56,  3.01it/s][34m[1mwandb[39m[22m: 429 encountered (Filestream rate limit exceeded, retrying in 2.3 seconds.), retrying request
Traceback (most recent call last):
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/trainer.py", line 527, in _inner_training_loop
    tr_loss_step = self.optimizer.step(closure)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/optimizers/zo_sgd.py", line 80, in step
    return self.zo_step(closure)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/optimizers/zo_sgd.py", line 122, in zo_step
    loss2 = closure()
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/trainer.py", line 764, in closure
    def closure(): return self.zo_forward(model, inputs)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/trainer.py", line 780, in zo_forward
    loss = self.compute_loss(model, inputs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/utils.py", line 103, in forward_wrap_with_option_len
    if any([x != num_options[0] for x in num_options]):
KeyboardInterrupt