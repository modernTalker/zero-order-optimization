2025-07-11 14:10:47,760 - INFO - Sample train set 1500/67349
2025-07-11 14:10:47,761 - INFO - ... including dev set 500 samples
2025-07-11 14:10:47,761 - INFO - Loading model with FP16...
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
37
2025-07-11 14:10:51,714 - INFO - Done with 3.95s
2025-07-11 14:10:51,992 - INFO - Dev samples: 500
2025-07-11 14:10:51,992 - INFO - Train samples: 1000
2025-07-11 14:10:51,992 - INFO - Eval sample length is 872
2025-07-11 14:10:51,993 - INFO - Tokenizing training samples...
2025-07-11 14:10:52,846 - INFO - Done with 0.85s
/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
2025-07-11 14:10:52,861 - INFO - ***** Running training *****
2025-07-11 14:10:52,861 - INFO -   Num examples = 1000
2025-07-11 14:10:52,861 - INFO -   Num Epochs = 318
2025-07-11 14:10:52,861 - INFO -   Instantaneous batch size per device = 16
2025-07-11 14:10:52,861 - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
2025-07-11 14:10:52,861 - INFO -   Gradient Accumulation steps = 1
2025-07-11 14:10:52,861 - INFO -   Total optimization steps = 20000
2025-07-11 14:10:52,862 - INFO -   Number of trainable parameters = 1315758080
  0%|                                                                                                                                                       | 0/20000 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
### layer-wise gradient sparsity = None
-------------------------- Training Epoch 0 --------------------------
  0%|                                                                                                                                             | 1/20000 [00:01<9:16:01,  1.67s/it]

  0%|                                                                                                                                             | 3/20000 [00:03<6:46:29,  1.22s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1082.885503768921, 'epoch': 0.03}

  0%|                                                                                                                                             | 5/20000 [00:06<6:18:33,  1.14s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1085.498571395874, 'epoch': 0.06}

  0%|                                                                                                                                             | 7/20000 [00:08<6:08:39,  1.11s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1080.8815956115723, 'epoch': 0.1}

  0%|                                                                                                                                             | 9/20000 [00:10<6:03:36,  1.09s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1077.0180225372314, 'epoch': 0.13}

  0%|                                                                                                                                            | 10/20000 [00:11<6:02:58,  1.09s/it]
{'loss': 1.0612, 'learning_rate': 0.5, 'epoch': 0.16}
{'peak_mem': 7.537271022796631, 'step_consumption': 1089.9121761322021, 'epoch': 0.16}

  0%|                                                                                                                                            | 12/20000 [00:13<6:04:09,  1.09s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1089.9674892425537, 'epoch': 0.19}

  0%|                                                                                                                                            | 14/20000 [00:15<6:03:27,  1.09s/it]

  0%|                                                                                                                                            | 16/20000 [00:17<6:01:44,  1.09s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1081.913709640503, 'epoch': 0.24}

  0%|▏                                                                                                                                           | 18/20000 [00:20<6:04:02,  1.09s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1085.3664875030518, 'epoch': 0.27}

  0%|▏                                                                                                                                           | 20/20000 [00:22<6:04:31,  1.09s/it]
{'peak_mem': 7.537271022796631, 'step_consumption': 1093.3234691619873, 'epoch': 0.3}
{'loss': 3.1309, 'learning_rate': 0.5, 'epoch': 0.32}
  0%|▏                                                                                                                                           | 21/20000 [00:23<6:06:33,  1.10s/it]Traceback (most recent call last):
  File "/home/rinya/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/rinya/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/rinya/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/rinya/zero-order-optimization/src/trainer.py", line 546, in _inner_training_loop
    tr_loss_step = self.optimizer.step(closure)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/home/rinya/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/rinya/zero-order-optimization/src/optimizers/jaguar_muon.py", line 78, in step
    param.grad = torch.zeros_like(param)
KeyboardInterrupt
{'peak_mem': 7.537271022796631, 'step_consumption': 1108.7980270385742, 'epoch': 0.33}