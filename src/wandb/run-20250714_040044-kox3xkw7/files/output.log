2025-07-14 04:00:56,337 - INFO - Sample train set 1500/67349
2025-07-14 04:00:56,338 - INFO - ... including dev set 500 samples
2025-07-14 04:00:56,338 - INFO - Loading model with FP16...
/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
10
2025-07-14 04:01:01,239 - INFO - Done with 4.90s
2025-07-14 04:01:01,562 - INFO - Dev samples: 500
2025-07-14 04:01:01,562 - INFO - Train samples: 1000
2025-07-14 04:01:01,562 - INFO - Eval sample length is 872
2025-07-14 04:01:01,563 - INFO - Tokenizing training samples...
2025-07-14 04:01:02,567 - INFO - Done with 1.00s
/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 740, in <module>
    main()
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 692, in main
    framework.train(train_samples, dev_samples if dev_samples is not None else eval_samples, eval_samples)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/run.py", line 577, in train
    trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/trainer.py", line 342, in _inner_training_loop
    self.optimizer = ZO_SGD(self.model.parameters(), lr=1e-3, eps=1e-2, momentum=0.0, gradient_sparsity=self.gradient_sparsity)
  File "/home/moderntalker/opt_projects/zero-order-optim/zero-order-optimization/src/optimizers/zo_sgd.py", line 56, in __init__
    SGD([group['params']], lr=group['lr'], momentum=group['momentum'])
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/sgd.py", line 26, in __init__
    super().__init__(params, defaults)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 266, in __init__
    self.add_param_group(cast(dict, param_group))
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/moderntalker/anaconda3/envs/zollm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 871, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is list